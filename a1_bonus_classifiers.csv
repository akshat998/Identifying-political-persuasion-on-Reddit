# 1. RandomForestClassifier
# This experiment varies the maximum depth of the estimator trees to find the depth that lead to the highest accuracy.
# From the result below, we can observe that the accuracy maximized at a depth=12, which equals to 0.467.
Working on depth 1
0.7603191599919228
[[1146   95  688   82]
 [ 509  365  961  121]
 [ 339  137 1458   92]
 [ 401  252 1146  208]]
accuracy for test is: 0.397125
recall for test is: [0.56986573843858779, 0.18660531697341512, 0.71964461994077, 0.10363726955655207]
precision for test is: [0.47849686847599165, 0.42991755005889282, 0.34281683517517048, 0.41351888667992048]
Working on depth 2
1.103430217001005
[[1116  250  607   38]
 [ 425  695  755   81]
 [ 292  356 1308   70]
 [ 403  541  973   90]]
accuracy for test is: 0.401125
recall for test is: [0.55494778717056192, 0.35531697341513291, 0.64560710760118456, 0.044843049327354258]
precision for test is: [0.49910554561717352, 0.37730727470141151, 0.35904474334339831, 0.32258064516129031]
Working on depth 3
1.5773436290037353
[[1219  221  293  278]
 [ 440  772  514  230]
 [ 322  399 1103  202]
 [ 397  599  631  380]]
accuracy for test is: 0.43425
recall for test is: [0.60616608652411741, 0.39468302658486709, 0.5444225074037512, 0.18933731938216244]
precision for test is: [0.51261564339781329, 0.38774485183324964, 0.43408107044470678, 0.34862385321100919]
Working on depth 4
2.371020384991425
[[1227  198  322  264]
 [ 411  808  547  190]
 [ 311  425 1109  181]
 [ 408  596  668  335]]
accuracy for test is: 0.434875
recall for test is: [0.61014420686225757, 0.41308793456032722, 0.54738400789733466, 0.16691579471848531]
precision for test is: [0.52057700466694956, 0.39861864824864329, 0.41912320483749055, 0.34536082474226804]
Working on depth 5
3.3129835089930566
[[1190  193  308  320]
 [ 330  778  516  332]
 [ 267  354 1089  316]
 [ 337  500  638  532]]
accuracy for test is: 0.448625
recall for test is: [0.59174540029835898, 0.39775051124744376, 0.53751233958538991, 0.26507224713502742]
precision for test is: [0.56026365348399243, 0.4263013698630137, 0.42689141513132106, 0.35466666666666669]
Working on depth 6
4.543051518005086
[[1192  213  291  315]
 [ 312  841  527  276]
 [ 245  394 1142  245]
 [ 338  583  623  463]]
accuracy for test is: 0.45475
recall for test is: [0.59273993038289408, 0.42995910020449896, 0.56367226061204345, 0.23069257598405579]
precision for test is: [0.57115476760900818, 0.41408173313638602, 0.44212156407278358, 0.35642802155504233]
Working on depth 7
5.877621874999022
[[1195  155  277  384]
 [ 271  809  468  408]
 [ 240  349 1085  352]
 [ 296  501  567  643]]
accuracy for test is: 0.4665
recall for test is: [0.59423172550969672, 0.41359918200408996, 0.53553800592300094, 0.32037867463876435]
precision for test is: [0.59690309690309695, 0.4459757442116869, 0.45264914476428869, 0.35982092893116957]
Working on depth 8
7.575605756996083
[[1181  182  289  359]
 [ 277  790  483  406]
 [ 215  321 1094  396]
 [ 270  485  592  660]]
accuracy for test is: 0.465625
recall for test is: [0.58727001491795128, 0.40388548057259716, 0.53998025666337612, 0.32884902840059793]
precision for test is: [0.60782295419454457, 0.44431946006749157, 0.4450772986167616, 0.36243822075782539]
Working on depth 9
9.245699203005643
[[1177  199  245  390]
 [ 285  792  458  421]
 [ 244  313 1100  369]
 [ 294  488  545  680]]
accuracy for test is: 0.468625
recall for test is: [0.5852809547488812, 0.40490797546012269, 0.54294175715695958, 0.33881415047334329]
precision for test is: [0.58850000000000002, 0.4419642857142857, 0.4684838160136286, 0.36559139784946237]
Working on depth 10
11.192878961999668
[[1166  187  254  404]
 [ 277  766  442  471]
 [ 239  324 1047  416]
 [ 271  481  517  738]]
accuracy for test is: 0.464625
recall for test is: [0.57981103928393829, 0.39161554192229037, 0.51678183613030604, 0.36771300448430494]
precision for test is: [0.5970302099334357, 0.43572241183162685, 0.46327433628318582, 0.3637259733859044]
Working on depth 11
13.262278419992072
[[1183  195  234  399]
 [ 260  777  419  500]
 [ 242  328 1018  438]
 [ 238  495  496  778]]
accuracy for test is: 0.4695
recall for test is: [0.58826454500248637, 0.39723926380368096, 0.50246791707798621, 0.38764324862979571]
precision for test is: [0.61518460738429537, 0.43286908077994429, 0.46977388094139361, 0.36784869976359336]
Working on depth 12
15.448747382994043
[[1138  223  235  415]
 [ 247  798  398  513]
 [ 221  358  992  455]
 [ 240  472  487  808]]
accuracy for test is: 0.467
recall for test is: [0.56588761810044752, 0.40797546012269936, 0.48963474827245806, 0.4025909317389138]
precision for test is: [0.61646803900325031, 0.43111831442463533, 0.46969696969696972, 0.36878137836604291]
Working on depth 13
17.92919288900157
[[1136  228  233  414]
 [ 278  761  427  490]
 [ 214  372  991  449]
 [ 229  500  482  796]]
accuracy for test is: 0.4605
recall for test is: [0.56489308801591254, 0.38905930470347649, 0.48914116485686082, 0.39661185849526659]
precision for test is: [0.61173936456650513, 0.40891993551853845, 0.46460384435067981, 0.37040483946021407]
Working on depth 14
20.44117111999367
[[1146  231  214  420]
 [ 274  760  399  523]
 [ 211  344  988  483]
 [ 237  530  463  777]]
accuracy for test is: 0.458875
recall for test is: [0.56986573843858779, 0.3885480572597137, 0.48766041461006909, 0.38714499252615847]
precision for test is: [0.6134903640256959, 0.40750670241286863, 0.47868217054263568, 0.35270086246028143]
Working on depth 15
23.261695178996888
[[1115  239  224  433]
 [ 266  781  374  535]
 [ 201  352  959  514]
 [ 225  514  466  802]]
accuracy for test is: 0.457125
recall for test is: [0.55445052212829438, 0.3992842535787321, 0.47334649555774927, 0.3996013951170902]
precision for test is: [0.61704482567791918, 0.41410392364793214, 0.47404844290657439, 0.35113835376532399]


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# MLPClassifier
# This experiment varies value of alpha, the L2 penalty (regularization term) parameter.
# From the result below, we can observe that the accuracy maximized at alpha = 0.1 and 0.75, which equals to 0.47.
# When the alpha value is too small, there could be a sign a overfitting, i.e. the L2 penalty is too small to accommodate changes in the dataset.

Working on alpha 1
25.943501150002703
[[ 828  681  234  268]
 [ 113 1302  318  223]
 [  99  738  940  249]
 [ 117 1063  439  388]]
accuracy for test is: 0.43225
recall for test is: [0.41173545499751368, 0.66564417177914115, 0.46396841066140176, 0.19332336821126059]
precision for test is: [0.71564390665514266, 0.34408033826638479, 0.48679440704298294, 0.34397163120567376]
Working on alpha 0.75
31.650068691000342
[[1104  307  225  375]
 [ 253  972  352  379]
 [ 193  482  940  411]
 [ 253  589  406  759]]
accuracy for test is: 0.471875
recall for test is: [0.54898060666335158, 0.49693251533742333, 0.46396841066140176, 0.37817638266068759]
precision for test is: [0.61231281198003329, 0.41361702127659572, 0.48881955278211131, 0.39449064449064447]
Working on alpha 0.5
36.445479444999364
[[942 356 217 496]
 [172 960 314 510]
 [134 470 915 507]
 [163 566 372 906]]
accuracy for test is: 0.465375
recall for test is: [0.46842366981601191, 0.49079754601226994, 0.45162882527147086, 0.45142002989536623]
precision for test is: [0.66761162296243803, 0.40816326530612246, 0.50330033003300334, 0.37453493178999586]
Working on alpha 0.25
42.99489114500466
[[1441  117  139  314]
 [ 655  647  284  370]
 [ 543  295  802  386]
 [ 621  354  342  690]]
accuracy for test is: 0.4475
recall for test is: [0.71655892590750869, 0.33077709611451944, 0.39585389930898324, 0.34379671150971597]
precision for test is: [0.44202453987730062, 0.45789101203113941, 0.51180599872367583, 0.39204545454545453]
Working on alpha 0.1
47.5628380249982
[[1303  149  136  423]
 [ 416  722  220  598]
 [ 324  358  746  598]
 [ 379  347  276 1005]]
accuracy for test is: 0.472
recall for test is: [0.64793635007458972, 0.36912065439672803, 0.36821322803553802, 0.50074738415545594]
precision for test is: [0.53798513625103217, 0.45812182741116753, 0.54136429608127723, 0.3830030487804878]
Working on alpha 0.05
52.66146694400231
[[1066  156  501  288]
 [ 207  669  822  258]
 [ 139  195 1432  260]
 [ 214  351  956  486]]
accuracy for test is: 0.456625
recall for test is: [0.53008453505718545, 0.34202453987730064, 0.70681145113524191, 0.24215246636771301]
precision for test is: [0.65559655596555966, 0.48796498905908098, 0.38587981676098088, 0.37616099071207432]
Working on alpha 0.025
59.69952123699477
[[ 769  815  146  281]
 [  70 1466  204  216]
 [  85  952  742  247]
 [  97 1234  269  407]]
accuracy for test is: 0.423
recall for test is: [0.3823968175037295, 0.74948875255623726, 0.36623889437314905, 0.20279023418036871]
precision for test is: [0.75318315377081291, 0.32818446384598166, 0.54518736223365172, 0.35360556038227631]
Working on alpha 0.01
62.985175744994194
[[1266   31  336  378]
 [ 448  203  718  587]
 [ 239   36 1310  441]
 [ 367   80  690  870]]
accuracy for test is: 0.456125
recall for test is: [0.62953754351069124, 0.10378323108384459, 0.64659427443237905, 0.43348281016442453]
precision for test is: [0.54568965517241375, 0.57999999999999996, 0.42894564505566468, 0.38224956063268895]

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# AdaBoostClassifier

# Below experiments varies the learning rate(default learning rate is 1 for this function).
# It can be found that, the best performance is found at a learning rate = 0.8, which gives an accuracy of 0.49325.
# If the learning rate is too small or too large, the model will be either in accurate or will be more likely to overfit.

Working on learning rate 0.1
67.77606387999549
[[1092  210  183  526]
 [ 222  807  384  543]
 [ 239  413  913  461]
 [ 197  560  431  819]]
accuracy for test is: 0.453875
recall for test is: [0.54301342615614123, 0.41257668711656442, 0.45064165844027643, 0.40807174887892378]
precision for test is: [0.624, 0.40552763819095478, 0.47776033490319203, 0.34865900383141762]
Working on learning rate 0.5
92.22551957999531
[[1195  191  216  409]
 [ 248  840  411  457]
 [ 222  336 1054  414]
 [ 207  485  503  812]]
accuracy for test is: 0.487625
recall for test is: [0.59423172550969672, 0.42944785276073622, 0.52023692003948663, 0.40458395615346288]
precision for test is: [0.63835470085470081, 0.45356371490280778, 0.48260073260073261, 0.3881453154875717]
Working on learning rate 0.8
116.24824623299355
[[1220  169  200  422]
 [ 243  823  380  510]
 [ 227  347  969  483]
 [ 231  426  416  934]]
accuracy for test is: 0.49325
recall for test is: [0.60666335156638485, 0.42075664621676889, 0.47828232971372164, 0.46537120079720978]
precision for test is: [0.63508589276418537, 0.46628895184135977, 0.49312977099236643, 0.39761600681140913]
Working on learning rate 1.0
139.11092928500148
[[1208  175  213  415]
 [ 273  812  365  506]
 [ 229  342  996  459]
 [ 240  427  433  907]]
accuracy for test is: 0.490375
recall for test is: [0.6006961710591745, 0.41513292433537835, 0.49160908193484698, 0.45191828599900347]
precision for test is: [0.61948717948717946, 0.4624145785876993, 0.4962630792227205, 0.3965894184521207]
Working on learning rate 1.5
163.50677359299152
[[1415  174  224  198]
 [ 433  773  406  344]
 [ 365  335 1024  302]
 [ 509  426  490  582]]
accuracy for test is: 0.47425
recall for test is: [0.70363003480855291, 0.39519427402862983, 0.50542941757156956, 0.28998505231689087]
precision for test is: [0.51983835415135926, 0.45257611241217799, 0.47761194029850745, 0.40813464235624125]
Working on learning rate 2.0
186.86629674000142
[[1695    0  316    0]
 [ 938    0 1018    0]
 [ 854    0 1172    0]
 [1033    0  974    0]]
accuracy for test is: 0.358375
recall for test is: [0.84286424664346093, 0.0, 0.57847976307996052, 0.0]
precision for test is: [0.375, 0, 0.33678160919540229, 0]

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# GaussianNB

# This classifier run gaussian naive bayes method. The result obtained has an adequate accuracy around 40 percent.
# The naive bayes method assumes statistical independence, meaning that the data from the Reddit corpus are independent from each other, this fits our model for political view prediction.
# This classifier does not consider feature interaction. As it can be seen from the recall values, the classification capability is not even among all categories.

Confusion Matrix
[[1231  362   86  332]
 [ 442 1083  103  328]
 [ 427  732  347  520]
 [ 505  801  124  577]]
accuracy for test is: 0.40475
recall for test is: [0.61213326703132764, 0.55368098159509205, 0.17127344521224086, 0.28749377179870456]
precision for test is: [0.47255278310940502, 0.3636668905305574, 0.52575757575757576, 0.32840068298235631]

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# DecisionTreeClassifier

1. max_features = 'sqrt', max_features=sqrt(n_features)
Confusion Matrix
0.5965626339893788
[[848 346 271 546]
 [286 622 428 620]
 [292 435 729 570]
 [284 504 417 802]]
accuracy for test is: 0.375125
recall for test is: [0.42168075584286424, 0.31799591002044991, 0.359822309970385, 0.3996013951170902]
precision for test is: [0.49590643274853802, 0.32616675406397483, 0.39512195121951221, 0.31599684791174154]

2. max_features = 'log2', max_features=log2(n_features)
Confusion Matrix
[[790 372 298 551]
 [336 565 420 635]
 [321 424 710 571]
 [324 496 411 776]]
accuracy for test is: 0.355125
recall for test is: [0.39283938339134761, 0.28885480572597139, 0.3504442250740375, 0.38664673642252118]
precision for test is: [0.44607566346696781, 0.30425417339795369, 0.38607939097335509, 0.30635609948677456]

3. max_features = None (linear), max_features=n_features.
Confusion Matrix
[[935 291 276 509]
 [303 606 409 638]
 [287 411 756 572]
 [272 489 434 812]]
accuracy for test is: 0.388625
recall for test is: [0.46494281452013925, 0.30981595092024539, 0.37314906219151034, 0.40458395615346288]
precision for test is: [0.52031163049526985, 0.337228714524207, 0.4032, 0.32082180956143819]

# Comment: The parameter max_features defines the maximun number of feature considered during the tree-split. According to our experiment, we can see that, the more number of features considered, the hight accuracy we can get.